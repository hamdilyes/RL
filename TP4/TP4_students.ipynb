{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c70d56",
   "metadata": {},
   "source": [
    "# TD Deep Q-Network\n",
    "\n",
    "\n",
    "HÃ©di Hadiji March 2023  \n",
    "Adapted from Odalric Ambryn-Maillard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1833c07",
   "metadata": {},
   "source": [
    "### Preliminary exercise: \n",
    "\n",
    "Below is the answer from ChatGPT to the query (March 2023)\n",
    "\n",
    "```\n",
    "    write a python implementation of q-learning\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa51176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 95.          94.24        94.9997568   94.24        94.24\n",
      "   76.        ]\n",
      " [100.          99.9999488   99.999744    99.999744    99.999744\n",
      "   99.99999795]\n",
      " [ 90.25        89.528       89.528       86.64        72.2\n",
      "   86.64      ]\n",
      " [ 95.          94.9696      94.24        94.99392     94.24\n",
      "   94.848     ]\n",
      " [100.          99.99872     99.84        99.99872     99.968\n",
      "   99.99872   ]\n",
      " [  0.           0.           0.           0.           0.\n",
      "    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the gridworld environment\n",
    "env = [[-1, -1, -1, -1, 0, -1],\n",
    "       [-1, -1, -1, 0, -1, 100],\n",
    "       [-1, -1, -1, 0, -1, -1],\n",
    "       [-1, 0, 0, -1, 0, -1],\n",
    "       [0, -1, -1, 0, -1, 100],\n",
    "       [-1, 0, -1, -1, 0, 100]]\n",
    "\n",
    "# Define the Q table\n",
    "Q = np.zeros((6, 6))\n",
    "\n",
    "# Define the hyperparameters\n",
    "alpha = 0.8\n",
    "gamma = 0.95\n",
    "epsilon = 0.1\n",
    "num_episodes = 1000\n",
    "\n",
    "# Q-learning algorithm\n",
    "for i in range(num_episodes):\n",
    "    state = np.random.randint(0, 6)\n",
    "    while state != 5:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(0, 6)\n",
    "        else:\n",
    "            action = np.argmax(Q[state, :])\n",
    "        next_state = np.argmax(env[state])\n",
    "        reward = env[state][next_state]\n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])\n",
    "        state = next_state\n",
    "\n",
    "# Print the final Q table\n",
    "print(Q)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77a6787e",
   "metadata": {},
   "source": [
    "**Question 0** Comment on the code.\n",
    "\n",
    "**Answer**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a86dc",
   "metadata": {},
   "source": [
    "# Tutorial - Deep Q-Learning \n",
    "\n",
    "Deep Q-Learning uses a neural network to approximate $Q$ functions. Hence, we usually refer to this algorithm as DQN (for *deep Q network*).\n",
    "\n",
    "The parameters of the neural network are denoted by $\\theta$. \n",
    "*   As input, the network takes a state $s$,\n",
    "*   As output, the network returns $Q_\\theta [a | s] = Q_\\theta (s,a) = Q(s, a, \\theta)$, the value of each action $a$ in state $s$, according to the parameters $\\theta$.\n",
    "\n",
    "\n",
    "The goal of Deep Q-Learning is to learn the parameters $\\theta$ so that $Q(s, a, \\theta)$ approximates well the optimal $Q$-function $Q^*(s, a) \\simeq Q_{\\theta^*} (s,a)$. \n",
    "\n",
    "In addition to the network with parameters $\\theta$, the algorithm keeps another network with the same architecture and parameters $\\theta^-$, called **target network**.\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "1.   At each time $t$, the agent is in state $s_t$ and has observed the transitions $(s_i, a_i, r_i, s_i')_{i=1}^{t-1}$, which are stored in a **replay buffer**.\n",
    "\n",
    "2.  Choose action $a_t = \\arg\\max_a Q_\\theta(s_t, a)$ with probability $1-\\varepsilon_t$, and $a_t$=random action with probability $\\varepsilon_t$. \n",
    "\n",
    "3. Take action $a_t$, observe reward $r_t$ and next state $s_t'$.\n",
    "\n",
    "4. Add transition $(s_t, a_t, r_t, s_t')$ to the **replay buffer**.\n",
    "\n",
    "4.  Sample a minibatch $\\mathcal{B}$ containing $B$ transitions from the replay buffer. Using this minibatch, we define the loss:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\sum_{(s_i, a_i, r_i, s_i') \\in \\mathcal{B}}\n",
    "\\left[\n",
    "Q(s_i, a_i, \\theta) -  y_i\n",
    "\\right]^2\n",
    "$$\n",
    "where the $y_i$ are the **targets** computed with the **target network** $\\theta^-$:\n",
    "\n",
    "$$\n",
    "y_i = r_i + \\gamma \\max_{a'} Q(s_i', a', \\theta^-).\n",
    "$$\n",
    "\n",
    "5. Update the parameters $\\theta$ to minimize the loss, e.g., with gradient descent (**keeping $\\theta^-$ fixed**): \n",
    "$$\n",
    "\\theta \\gets \\theta - \\eta \\nabla_\\theta L(\\theta)\n",
    "$$\n",
    "where $\\eta$ is the optimization learning rate. \n",
    "\n",
    "6. Every $N$ transitions ($t\\mod N$ = 0), update target parameters: $\\theta^- \\gets \\theta$.\n",
    "\n",
    "7. $t \\gets t+1$. Stop if $t = T$, otherwise go to step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3192463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hedihadiji/opt/anaconda3/envs/pyRL/lib/python3.9/site-packages/gymnasium/envs/registration.py:521: UserWarning: \u001b[33mWARN: Overriding environment GymV26Environment-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/Users/hedihadiji/opt/anaconda3/envs/pyRL/lib/python3.9/site-packages/gymnasium/envs/registration.py:521: UserWarning: \u001b[33mWARN: Overriding environment GymV22Environment-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8da2ac68",
   "metadata": {},
   "source": [
    "If necessary: install pytorch by running \n",
    "\n",
    "`pip3 install torch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ea50e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python --version = 3.9.15 (main, Nov 24 2022, 08:29:02) \n",
      "[Clang 14.0.6 ]\n",
      "torch.__version__ = 2.0.0\n",
      "np.__version__ = 1.22.4\n",
      "gym.__version__ = 0.27.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"python --version = {sys.version}\")\n",
    "print(f\"torch.__version__ = {torch.__version__}\")\n",
    "print(f\"np.__version__ = {np.__version__}\")\n",
    "print(f\"gym.__version__ = {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117afb1",
   "metadata": {},
   "source": [
    "## Torch 101\n",
    "\n",
    ">\"The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities. \n",
    "[...] provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions.\" \n",
    "[PyTorch](https://pytorch.org/docs/stable/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f94aaf",
   "metadata": {},
   "source": [
    "### Variable types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ea2eaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero_torch is of type <class 'torch.Tensor'>\n",
      "\n",
      "Float:\n",
      " tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "Int:\n",
      " tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]])\n",
      "Bool:\n",
      " tensor([[False, False],\n",
      "        [False, False],\n",
      "        [False, False]])\n",
      "\n",
      "View new shape... tensor([[0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "Algebraic operations are overloaded:\n",
      " tensor([[ 0.8757, -0.7921],\n",
      "        [ 0.7620,  1.3835],\n",
      "        [ 0.3071, -0.7747]]) \n",
      "+\n",
      " tensor([[ 0.2853,  0.0319],\n",
      "        [-0.2896,  0.9068],\n",
      "        [ 0.2024,  0.9950]]) \n",
      "=\n",
      " tensor([[ 1.1611, -0.7602],\n",
      "        [ 0.4724,  2.2903],\n",
      "        [ 0.5095,  0.2202]])\n"
     ]
    }
   ],
   "source": [
    "# Very similar syntax to numpy.\n",
    "zero_torch = torch.zeros((3, 2))\n",
    "\n",
    "print('zero_torch is of type {:s}'.format(str(type(zero_torch))))\n",
    "\n",
    "# Torch -> Numpy: simply call the numpy() method.\n",
    "zero_np = np.zeros((3, 2))\n",
    "assert (zero_torch.numpy() == zero_np).all()\n",
    "\n",
    "# Numpy -> Torch: simply call the corresponding function on the np.array.\n",
    "zero_torch_float = torch.FloatTensor(zero_np)\n",
    "print('\\nFloat:\\n', zero_torch_float)\n",
    "zero_torch_int = torch.LongTensor(zero_np)\n",
    "print('Int:\\n', zero_torch_int)\n",
    "zero_torch_bool = torch.BoolTensor(zero_np)\n",
    "print('Bool:\\n', zero_torch_bool)\n",
    "\n",
    "# Reshape\n",
    "print('\\nView new shape...', zero_torch.view(1, 6))\n",
    "# Note that print(zero_torch.reshape(1, 6)) would work too.\n",
    "# The difference is in how memory is handled (view imposes contiguity).\n",
    "\n",
    "# Algebra\n",
    "a = torch.randn((3, 2))\n",
    "b = torch.randn((3, 2))\n",
    "print('\\nAlgebraic operations are overloaded:\\n', a, '\\n+\\n', b, '\\n=\\n', a+b )\n",
    "\n",
    "# More generally, torch shares the syntax of many attributes and functions with Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7f1191",
   "metadata": {},
   "source": [
    "### Gradient management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29a2a715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "Initial guess: tensor([-1.2694])\n",
      "Final estimate: tensor([1.9812])\n",
      "The final estimate should be close to tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor is a similar yet more complicated data structure than np.array.\n",
    "# It is basically a static array of number but may also contain an overlay to \n",
    "# handle automatic differentiation (i.e keeping track of the gradient and which \n",
    "# tensors depend on which).\n",
    "# To access the static array embedded in a tensor, simply call the detach() method\n",
    "print(zero_torch.detach())\n",
    "\n",
    "# When inside a function performing automatic differentiation (basically when training \n",
    "# a neural network), never use detach() otherwise meta information regarding gradients\n",
    "# will be lost, effectively freezing the variable and preventing backprop for it. \n",
    "# However when returning the result of training, do use detach() to save memory \n",
    "# (the naked tensor data uses much less memory than the full-blown tensor with gradient\n",
    "# management, and is much less prone to mistake such as bad copy and memory leak).\n",
    "\n",
    "# We will solve theta * x = y in theta for x=1 and y=2\n",
    "x = torch.ones(1)\n",
    "y = 2 * torch.ones(1)\n",
    "\n",
    "# Actually by default torch does not add the gradient management overlay\n",
    "# when declaring tensors like this. To force it, add requires_grad=True.\n",
    "theta = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Optimisation routine\n",
    "# (Adam is a sophisticated variant of SGD, with adaptive step).\n",
    "optimizer = optim.Adam(params=[theta], lr=0.1)\n",
    "\n",
    "# Loss function\n",
    "print('Initial guess:', theta.detach())\n",
    "\n",
    "for _ in range(100):\n",
    "    # By default, torch accumulates gradients in memory.\n",
    "    # To obtain the desired gradient descent beahviour,\n",
    "    # just clean the cached gradients using the following line:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Quadratic loss (* and ** are overloaded so that torch\n",
    "    # knows how to differentiate them)\n",
    "    loss = (y - theta * x) ** 2\n",
    "    \n",
    "    # Apply the chain rule to automatically compute gradients\n",
    "    # for all relevant tensors.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Run one step of optimisation routine.\n",
    "    optimizer.step()\n",
    "    \n",
    "print('Final estimate:', theta.detach())\n",
    "print('The final estimate should be close to', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c38d6dd",
   "metadata": {},
   "source": [
    "# Setting up the DQN agent\n",
    "\n",
    "### 1 - Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd61940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, terminated, next_state):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = (state, action, reward, terminated, next_state)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.choices(self.memory, k=batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# create instance of replay buffer\n",
    "#replay_buffer = ReplayBuffer(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a64a096",
   "metadata": {},
   "source": [
    "### 2 - Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0decdfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic neural net.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eaf259",
   "metadata": {},
   "source": [
    "### 2.5 - Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae4de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Skeleton: \n",
    "    def __init__(self,\n",
    "                action_space,\n",
    "                observation_space,\n",
    "                gamma,\n",
    "                batch_size,\n",
    "                buffer_capacity,\n",
    "                update_target_every, \n",
    "                epsilon_start, \n",
    "                decrease_epsilon_factor, \n",
    "                epsilon_min,\n",
    "                learning_rate,\n",
    "                ): \n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.update_target_every = update_target_every\n",
    "        \n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.decrease_epsilon_factor = decrease_epsilon_factor # larger -> more exploration\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        ** TO BE IMPLEMENTED LATER**\n",
    "\n",
    "        Return action according to an epsilon-greedy exploration policy\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def update(self, *data):\n",
    "        \"\"\"\n",
    "        ** TO BE IMPLEMENTED LATER **\n",
    "\n",
    "        Updates the buffer and the network(s)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_q(self, state):\n",
    "        \"\"\"\n",
    "        Compute Q function for a states\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            output = self.q_net.forward(state_tensor) # shape (1,  n_actions)\n",
    "        return output.numpy()[0]  # shape  (n_actions)\n",
    "    \n",
    "    def decrease_epsilon(self):\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_start - self.epsilon_min) * (\n",
    "                        np.exp(-1. * self.n_eps / self.decrease_epsilon_factor ) )\n",
    "    \n",
    "    def reset(self):\n",
    "        hidden_size = 128\n",
    "        \n",
    "        obs_size = self.observation_space.shape[0]\n",
    "        n_actions = self.action_space.n\n",
    "        \n",
    "        self.buffer = ReplayBuffer(self.buffer_capacity)\n",
    "        self.q_net =  Net(obs_size, hidden_size, n_actions)\n",
    "        self.target_net = Net(obs_size, hidden_size, n_actions)\n",
    "        \n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(params=self.q_net.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        self.epsilon = self.epsilon_start\n",
    "        self.n_steps = 0\n",
    "        self.n_eps = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent: \n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.action_space = action_space\n",
    "        return\n",
    "    \n",
    "    def get_action(self, state, *args):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "    def update(self, *data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b72807",
   "metadata": {},
   "source": [
    "## Implementing the DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ababe",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "\n",
    "Implement the `get_action` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_SkeletonI(DQN_Skeleton):   \n",
    "    def get_action(self, state, epsilon=None):\n",
    "        \"\"\"\n",
    "        ** TO BE IMPLEMENTED NOW**\n",
    "\n",
    "        Return action according to an epsilon-greedy exploration policy\n",
    "        \"\"\"\n",
    "        if epsilon is None: \n",
    "            epsilon = self.epsilon\n",
    "            \n",
    "        # Your code here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3302b69f",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "Implement the `eval_dqn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f1d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_agent(agent, env, n_sim=5):\n",
    "    \"\"\"\n",
    "    ** TO BE IMPLEMENTED **\n",
    "    \n",
    "    Monte Carlo evaluation of DQN agent.\n",
    "\n",
    "    Repeat n_sim times:\n",
    "        * Run the DQN policy until the environment reaches a terminal state (= one episode)\n",
    "        * Compute the sum of rewards in this episode\n",
    "        * Store the sum of rewards in the episode_rewards array.\n",
    "    \"\"\"\n",
    "    env_copy = deepcopy(env)\n",
    "    episode_rewards = np.zeros(n_sim)\n",
    "    # Your code here\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5703dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "agent = RandomAgent(env.observation_space, env.action_space)\n",
    "\n",
    "def run_one_episode(env, agent, display=True):\n",
    "    display_env = deepcopy(env)\n",
    "    done = False\n",
    "    state, _ = display_env.reset()\n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(state, 0)\n",
    "        state, reward, done, _, _ = display_env.step(action)\n",
    "        rewards += reward\n",
    "        if display: \n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(display_env.render())\n",
    "            plt.show()\n",
    "    if display:\n",
    "        display_env.close()\n",
    "    print(f'Episode length {rewards}')\n",
    "    \n",
    "run_one_episode(env, agent, display=True)\n",
    "print(f'Average over 5 runs : {np.mean(eval_agent(agent, env))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f2b0a",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "Implement the `update` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1e6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(DQN_SkeletonI):   \n",
    "    def update(self, state, action, reward, terminated, next_state):\n",
    "        \"\"\"\n",
    "        ** TO BE COMPLETED **\n",
    "        \"\"\"\n",
    "\n",
    "        # add data to replay buffer\n",
    "        self.buffer.push(torch.tensor(state).unsqueeze(0), \n",
    "                           torch.tensor([[action]], dtype=torch.int64), \n",
    "                           torch.tensor([reward]), \n",
    "                           torch.tensor([terminated], dtype=torch.int64), \n",
    "                           torch.tensor(next_state).unsqueeze(0),\n",
    "                          )\n",
    "\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return np.inf\n",
    "\n",
    "        # get batch\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        # Compute loss - TO BE IMPLEMENTED!\n",
    "        # Hint: use the gather method from torch.\n",
    "\n",
    "        \n",
    "        # Optimize the model \n",
    "        \n",
    "\n",
    "        return loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0152c68",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "Train a DQN on the `env` environment.\n",
    "*Hint* The mean reward after training should be close to 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "\n",
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "gamma = 0.99\n",
    "batch_size = 12\n",
    "buffer_capacity = 10_000\n",
    "update_target_every = 32\n",
    "\n",
    "epsilon_start = 0.1\n",
    "decrease_epsilon_factor = 1000\n",
    "epsilon_min = 0.05\n",
    "\n",
    "learning_rate = 2\n",
    "\n",
    "arguments = (action_space,\n",
    "            observation_space,\n",
    "            gamma,\n",
    "            batch_size,\n",
    "            buffer_capacity,\n",
    "            update_target_every, \n",
    "            epsilon_start, \n",
    "            decrease_epsilon_factor, \n",
    "            epsilon_min,\n",
    "            learning_rate,\n",
    "        )\n",
    "\n",
    "N_episodes = 400\n",
    "\n",
    "agent = DQN(*arguments)\n",
    "\n",
    "\n",
    "def train(env, agent, N_episodes, eval_every=10, reward_threshold=300):\n",
    "    total_time = 0\n",
    "    state, _ = env.reset()\n",
    "    losses = []\n",
    "    for ep in range(N_episodes):\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        while not done: \n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            loss_val = agent.update(state, action, reward, terminated, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            losses.append(loss_val)\n",
    "\n",
    "            done = terminated or truncated\n",
    "            total_time += 1\n",
    "\n",
    "        if ((ep+1)% eval_every == 0):\n",
    "            rewards = eval_agent(agent, env)\n",
    "            print(\"episode =\", ep+1, \", reward = \", np.mean(rewards))\n",
    "            if np.mean(rewards) >= reward_threshold:\n",
    "                break\n",
    "                \n",
    "    return losses\n",
    "\n",
    "    \n",
    "# Run the training loop\n",
    "losses = train(env, agent, N_episodes)\n",
    "\n",
    "plt.plot(losses)\n",
    "\n",
    "# Evaluate the final policy\n",
    "rewards = eval_agent(agent, env, 20)\n",
    "print(\"\")\n",
    "print(\"mean reward after training = \", np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898bfde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rewards after training = \", eval_agent(agent, env))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90708d1",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "\n",
    "Experiment the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_one_episode(env, agent, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb94bc",
   "metadata": {},
   "source": [
    "### Experiments: Do It Yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eae9fdf",
   "metadata": {},
   "source": [
    "Remember the set of parameters:\n",
    "```\n",
    "# Environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "buffer_capacity = 10_000\n",
    "update_target_every = 32\n",
    "\n",
    "epsilon_start = 0.9\n",
    "decrease_epsilon_factor = 200\n",
    "epsilon_min = 0.05\n",
    "\n",
    "learning_rate = 1e-2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba2f8e",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "\n",
    "Craft an experiment and study the influence of the `BUFFER_CAPACITY` on the learning process (speed of *convergence*, training curves...) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1533f",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "\n",
    "Craft an experiment and study the influence of the `UPDATE_TARGET_EVERY` on the learning process (speed of *convergence*, training curves...) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f197f12",
   "metadata": {},
   "source": [
    "#### Question 8\n",
    "\n",
    "If you have the computer power to do so, try to do a grid search on those two hyper-parameters and comment the results. Otherwise, study the influence of another hyper-parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc76ffb",
   "metadata": {},
   "source": [
    "## Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b35d83",
   "metadata": {},
   "source": [
    "It is natural to use a function approximator like a neural network to approximate the $Q$ function in a continuous environment. Another natural but unscalable way to do handle continuous state-action space is **discretization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1525f",
   "metadata": {},
   "source": [
    "Discretize the environment of your choice (cartpole or mountain car or both) and run one of the algorithms that you know to compute an approximation of the optimal $Q$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c32ad0",
   "metadata": {},
   "source": [
    "Once you are satisfied with your results, you may plot the *optimal phase diagram* of the system. For instance, you may get something like this for the mountain car environment.\n",
    "![Phase diagram](./phase_plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff4d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Everything! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for decrease_factor in [1, 100, 2000, 10_000]:\n",
    "    arguments = (action_space,\n",
    "                observation_space,\n",
    "                gamma,\n",
    "                batch_size,\n",
    "                buffer_capacity,\n",
    "                update_target_every, \n",
    "                0.9, \n",
    "                decrease_factor, \n",
    "                0.05,\n",
    "                learning_rate,\n",
    "            )\n",
    "\n",
    "    test_agent = DQN(*arguments)\n",
    "    epss = []\n",
    "    N = 1000\n",
    "\n",
    "    for _ in range(N):\n",
    "        epss.append(test_agent.epsilon)\n",
    "        test_agent.decrease_epsilon()\n",
    "        test_agent.n_eps += 1\n",
    "\n",
    "    plt.plot(epss, label=f'Decrease Factor: {decrease_factor}')\n",
    "plt.title('Epsilon vs N')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
