{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BILBZu2zz3pf"
   },
   "source": [
    "# RL Lab 03 - Part II- Control of finite unknown MDPs\n",
    "\n",
    "CentraleSupélec Mention IA 2022-2023  \n",
    "Hédi Hadiji "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from copy import deepcopy\n",
    "\n",
    "import plotting\n",
    "from utilities import * \n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inline display trick\n",
    "import time\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "icPxbY3Kz3pj"
   },
   "source": [
    "## 1. Monte-Carlo Control\n",
    "\n",
    "Let us rerun the code from part I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xx0DnPdtz3pm"
   },
   "outputs": [],
   "source": [
    "from gymnasium.spaces import Discrete\n",
    "\n",
    "class LinearEnv:\n",
    "    \"\"\"\n",
    "    State Index:   [ 0    1    2    3    4    5    6 ]\n",
    "    State Label:   [ .    A    B    C    D    E    . ]\n",
    "    Type:          [ T    .    .    S    .    .    T ]    \n",
    "    \"\"\"\n",
    "    V_true =       [0.0, 1/6, 2/6, 3/6, 4/6, 5/6, 0.0] # true values for gamma=1\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.observation_space = Discrete(7)\n",
    "        self.action_space = Discrete(2)\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = 3\n",
    "        self._terminated = False\n",
    "        return self._state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0: \n",
    "            self._state -= 1\n",
    "        if action == 1: \n",
    "            self._state += 1\n",
    "            \n",
    "        reward = 0\n",
    "        if self._state < 1:\n",
    "            self._terminated = True\n",
    "        if self._state > 5: \n",
    "            self._terminated = True\n",
    "            reward = 1\n",
    "                \n",
    "        return self._state, reward, self._terminated, False, {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build upon the Monte-Carlo policy evaluation agent that we coded earlier, to get a controller. \n",
    "\n",
    "\n",
    "**Question** (Do not scroll down) What is the key new ingredient that we need to implement to switch from policy evaluation to control? \n",
    "\n",
    "Answer: \n",
    "\n",
    "Policy improvement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the algorithm: \n",
    "\n",
    "Start with an arbitrary policy.\n",
    "\n",
    "Run policy Monte-Carlo policy evaluation for N rounds\n",
    "\n",
    "Change the policy to eps-greedy with respect to the estimated values. \n",
    "\n",
    "Repeat. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What is the issue with our previous Policy Evaluation algorithm if we want to compute the $\\epsilon$-greedy policy associated to it? \n",
    "\n",
    "Answer: \n",
    "\n",
    "We computed state-value estimates, but we need action-values to compute the greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCController: \n",
    "    \"\"\"\n",
    "        Monte-Carlo control\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 action_space,\n",
    "                 observation_space,\n",
    "                 gamma=0.99, \n",
    "                 eps_init=.5, \n",
    "                 eps_min=1e-5,\n",
    "                 eps_step=1e-3,\n",
    "                 episodes_between_greedyfication=500,\n",
    "                 name='MC Controller'\n",
    "                ):\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.gamma = gamma\n",
    "        self.name = name\n",
    "\n",
    "        self.eps = eps_init\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_step = eps_step\n",
    "\n",
    "        self.episodes_between_greedyfication = episodes_between_greedyfication\n",
    "        self.episode_counter = 0\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def eps_greedy(self, obs, eps=None):\n",
    "        if eps is None: \n",
    "            eps = self.eps\n",
    "\n",
    "        # Return a greedy action wrt the action values estimates.\n",
    "\n",
    "        if np.random.random() < self.eps: \n",
    "            return self.action_space.sample()\n",
    "        else:\n",
    "            b = self.q_values[obs]\n",
    "            return np.random.choice(np.flatnonzero(b == np.max(b))) # argmax with random tie-breaking\n",
    "            #return np.argmax(b)\n",
    "        \n",
    "    def get_action(self, obs): \n",
    "        return self.eps_greedy(obs)\n",
    "        \n",
    "    def update(self, obs, action, reward, terminated, next_obs):\n",
    "        self.current_episode_rewards.append(reward)\n",
    "        self.current_episode_obs.append((obs, action))\n",
    "        \n",
    "        if terminated:\n",
    "            self._end_of_episode_update()\n",
    "\n",
    "    def _end_of_episode_update(self):\n",
    "        current_episode_returns = rewardseq_to_returns(self.current_episode_rewards, self.gamma)\n",
    "        seen = []\n",
    "        for i, (state, action) in enumerate(self.current_episode_obs): \n",
    "            if (state, action) not in seen: \n",
    "                seen.append((state, action))\n",
    "                return_value = current_episode_returns[i]\n",
    "                \n",
    "                n = self.number_of_values_in_estimate[state, action]\n",
    "                self.values_estimates[state][action] = n / (n + 1) * self.values_estimates[state][action] + 1 / (n+1) * return_value\n",
    "                self.number_of_values_in_estimate[state, action] += 1\n",
    "                            \n",
    "        self.current_episode_rewards = []\n",
    "        self.current_episode_obs = []\n",
    "\n",
    "        self.episode_counter += 1 \n",
    "        \n",
    "        if self.episode_counter % self.episodes_between_greedyfication == 0:\n",
    "            new_q_values = defaultdict(lambda: np.zeros(self.action_space.n))\n",
    "            for state in self.values_estimates: \n",
    "                new_q_values[state] = self.values_estimates[state]\n",
    "            self.reset(q_values=new_q_values)\n",
    "            self.epsilon_decay()\n",
    "    \n",
    "    def epsilon_decay(self):\n",
    "        self.eps = max(self.eps - self.eps_step, self.eps_min)\n",
    "            \n",
    "    def reset(self, q_values=None):\n",
    "        self.current_episode_rewards = []\n",
    "        self.current_episode_obs = []\n",
    "\n",
    "        if q_values is None:\n",
    "            self.q_values = defaultdict(lambda: np.zeros(self.action_space.n))\n",
    "        else: \n",
    "            self.q_values = q_values\n",
    "\n",
    "        self.values_estimates = defaultdict(lambda: np.zeros(self.action_space.n))\n",
    "        self.number_of_values_in_estimate = defaultdict(lambda : 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to where the decaying in epsilon is done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = LinearEnv()\n",
    "gamma = .999\n",
    "N_episodes = 100_000\n",
    "\n",
    "episodes_between_greedyfication = 20_000\n",
    "\n",
    "eps_init = .1\n",
    "eps_step= eps_init * episodes_between_greedyfication / N_episodes\n",
    "\n",
    "agent = MCController(env.action_space, env.observation_space, eps_init=eps_init, eps_step=eps_step, gamma=gamma, episodes_between_greedyfication=episodes_between_greedyfication)\n",
    "env, agent = run_N_episodes(env, agent, N_episodes=N_episodes)\n",
    "\n",
    "plotting.plot_RW([np.max(agent.q_values[obs]) for obs in range(7)])\n",
    "#plt.plot([np.max(agent.q_values[obs]) for obs in range(7)])\n",
    "#plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What exactly did we plot above? Is it the value of the policy?  \n",
    "\n",
    "**Answer:**  \n",
    "We plotted the maximum of the action values estimates \n",
    "$$\n",
    "    \\max_{a \\in \\mathcal A} q(s, a)\n",
    "$$\n",
    "where $q$ is an estimator of the optimal action-value. The final policy is ($\\eps-)greedy with respect to $q$.\n",
    "\n",
    "If the estimation has gone well, then our plot should be a `reasonable' estimator for $V_\\star$ since\n",
    "$$\n",
    "    \\max_{a \\in \\mathcal A} q_\\star(s, a) = V_\\star(s) \n",
    "$$\n",
    "\n",
    "However, if the estimation has gone wrong then the policy may not be optimal. In that case, the $V$ we plotted is neither $V_\\star$, nor the true value of the policy we are playing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Q-learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"img/q-learning-description.png\" \n",
    "     width=\"50%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner(): \n",
    "    \"\"\"\n",
    "        Stores the data and computes the observed returns.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 action_space, \n",
    "                 observation_space, \n",
    "                 gamma=0.99, \n",
    "                 lr=0.1,\n",
    "                 eps_init=.5, \n",
    "                 eps_min=1e-5,\n",
    "                 eps_step=1-3,\n",
    "                 name='Q-learning'):\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.eps = eps_init\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_step = eps_step\n",
    "\n",
    "        self.name = name\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def eps_greedy(self, obs, eps=None):\n",
    "        if eps is None: \n",
    "            eps = self.eps\n",
    "\n",
    "        if np.random.random() < self.eps: \n",
    "            return self.action_space.sample()\n",
    "        else:\n",
    "            b = self.q_values[obs]\n",
    "            return np.random.choice(np.flatnonzero(b == np.max(b))) # argmax with random tie-breaking\n",
    "            #return np.argmax(b)\n",
    "        \n",
    "    def get_action(self, obs): \n",
    "        return self.eps_greedy(obs)\n",
    "        \n",
    "    def update(self, obs, action, reward, terminated, next_obs):\n",
    "        # update the q-values\n",
    "\n",
    "        estimate_value_at_next_state = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        new_estimate = reward + self.gamma * estimate_value_at_next_state\n",
    "        \n",
    "        self.q_values[obs][action] = (\n",
    "            (1 - self.lr) * self.q_values[obs][action] \n",
    "            + self.lr * new_estimate\n",
    "        )\n",
    "        \n",
    "        self.epsilon_decay()\n",
    "        \n",
    "    def epsilon_decay(self):\n",
    "        self.eps = max(self.eps - self.eps_step, self.eps_min)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.q_values = defaultdict(lambda: np.zeros(self.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = LinearEnv()\n",
    "lr = 0.1\n",
    "gamma = 0.9999\n",
    "\n",
    "N_episodes = 100\n",
    "eps_init = .5\n",
    "eps_step = 10 / N_episodes\n",
    "\n",
    "agent = QLearner(env.action_space, env.observation_space, gamma=gamma, lr=lr)\n",
    "env, agent = run_N_episodes(env, agent, N_episodes=N_episodes)\n",
    "\n",
    "plotting.plot_RW([np.max(agent.q_values[obs]) for obs in range(7)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Environments\n",
    "\n",
    "Now that we have tested our two algorithms on a toy environment. Let us try on more sophisticated ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "lr = 0.01\n",
    "gamma = .99\n",
    "N_episodes = 100_000\n",
    "\n",
    "eps_init = .5\n",
    "eps_step = 2 * eps_init / N_episodes\n",
    "\n",
    "agent = QLearner(env.action_space, env.observation_space, gamma=gamma, lr=lr)\n",
    "env, agent = run_N_episodes(env, agent, N_episodes=N_episodes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Can you find values of hyperparameters that converge to the optimal policy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "# Values to get a close-to-optimal policy: \n",
    "\n",
    "# lr = 0.001  \n",
    "# gamma = .9999  \n",
    "# N_episodes = 1_000_000  \n",
    "\n",
    "# eps_init = .5  \n",
    "# eps_step = 2 * eps_init / N_episodes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_grid, policy_grid = plotting.create_grids(agent, usable_ace=True)\n",
    "plotting.create_plots(value_grid, policy_grid, title=\"With usable Ace\")\n",
    "\n",
    "value_grid, policy_grid = plotting.create_grids(agent, usable_ace=False)\n",
    "plotting.create_plots(value_grid, policy_grid, title=\"Without usable Ace\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now do the same with the Monte-Carlo Controller\n",
    "\n",
    "**(Open) question** (it's not super interesting but I don't have an answer)\n",
    "Can you find a set of hyperparameters to make it converge to the optimal policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "gamma = .9999\n",
    "N_episodes = 100_000\n",
    "episodes_between_greedyfication = 30_000\n",
    "\n",
    "eps_init = .5\n",
    "eps_step= eps_init * episodes_between_greedyfication / N_episodes\n",
    "\n",
    "agent = MCController(env.action_space, env.observation_space, eps_init=eps_init, eps_step=eps_step, gamma=gamma, episodes_between_greedyfication=episodes_between_greedyfication)\n",
    "env, agent = run_N_episodes(env, agent, N_episodes=N_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_grid, policy_grid = plotting.create_grids(agent, usable_ace=True)\n",
    "plotting.create_plots(value_grid, policy_grid, title=\"With usable Ace\")\n",
    "\n",
    "value_grid, policy_grid = plotting.create_grids(agent, usable_ace=False)\n",
    "plotting.create_plots(value_grid, policy_grid, title=\"Without usable Ace\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**  Which do you prefer?  \n",
    "\n",
    "Answer: \n",
    "\n",
    "It's your choice. Monte-Carlo is harder to tune here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Policies\n",
    "\n",
    "Gymnasium provides wrappers to environments to record data. The RecordEpisodeStatistics wrapper adds (for example) a return_queue attribute to the environment that stores the returns received. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(agent, env, n_episodes=100): # Test the greedy policy for the q-function learned by the agent\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "    for episode in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.eps_greedy(obs, eps=0)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            obs = next_obs\n",
    "\n",
    "    return env\n",
    "\n",
    "def plot_test_stats(env, agent, plot=False):\n",
    "    returns = np.array(env.return_queue).flatten()\n",
    "    N_ep = len(returns)\n",
    "    print('Mean return over n={} episodes : {} +/- {}'.format(N_ep, np.mean(returns), np.std(returns)/np.sqrt(N_ep)))\n",
    "    ep_lengths = np.array(env.length_queue).flatten()\n",
    "    print('Mean episode length over n={} episodes : {}'.format(N_ep, np.mean(ep_lengths), np.std(ep_lengths)/np.sqrt(N_ep)))\n",
    "    \n",
    "    if plot: \n",
    "        fig, axs = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "        axs[0].set_title(\"Episode rewards\")\n",
    "        axs[0].axvline(x=np.mean(returns), linestyle='--', alpha=0.5)\n",
    "        axs[0].hist(returns)\n",
    "\n",
    "        axs[1].set_title(\"Episode lengths\")\n",
    "        axs[1].hist(ep_lengths)\n",
    "        axs[1].axvline(x=np.mean(ep_lengths))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "lr = 0.1\n",
    "gamma = .99\n",
    "N_episodes = 500_000\n",
    "\n",
    "eps_init = .1\n",
    "eps_step = 10 / N_episodes\n",
    "\n",
    "agent = QLearner(env.action_space, env.observation_space, gamma=gamma, lr=lr)\n",
    "env, agent = run_N_episodes(env, agent, N_episodes=N_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = test(agent, env, n_episodes=100)\n",
    "plot_test_stats(env, agent, plot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an agent on the Frozen Lake environemnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")\n",
    "lr = 0.01\n",
    "gamma = .99\n",
    "\n",
    "N_episodes = 100_000\n",
    "eps_init = .1\n",
    "eps_step = 2 * eps_init / N_episodes\n",
    "\n",
    "agent = QLearner(env.action_space, env.observation_space, eps_init=eps_init, eps_step=eps_step, gamma=gamma, lr=lr)\n",
    "env = test(agent, env, n_episodes=100)\n",
    "plot_test_stats(env, agent, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")\n",
    "env, agent = run_N_episodes(env, agent, N_episodes=N_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")\n",
    "env = test(agent, env, n_episodes=100)\n",
    "plot_test_stats(env, agent, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "done = False\n",
    "while not done: \n",
    "    action = agent.eps_greedy(obs, eps=0)\n",
    "    obs, _, terminated, truncated, _ = env.step(action)\n",
    "    \n",
    "    done = terminated or truncated\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(env.render())\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "#(0, 1, 2, 3) is  (left, down, up, right)\n",
    "\n",
    "value_grid = np.zeros((4, 4))\n",
    "for i, j in product(range(4), range(4)):\n",
    "        value_grid[i, j] = np.argmax(agent.q_values[i * 4 + j])\n",
    "        \n",
    "fig = plt.figure(figsize=plt.figaspect(.5))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "ax1 = sns.heatmap(value_grid, annot=True, cbar=False, \n",
    "                  cmap=ListedColormap(['white']), linecolor='black', linewidth=1,\n",
    "                 annot_kws={\"size\": 20})\n",
    "\n",
    "ax1.tick_params(left=False, bottom=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"img/almost_optimal_fr_lake.png\" \n",
    "     width=\"30%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RL_Lab_03_Monte_Carlo_Temporal_Difference_Prediction.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
